import warnings

import numpy as np
from abcpy.continuousmodels import ProbabilisticModel, Continuous, InputConnector
from abcpy.statistics import Statistics
from scipy.stats import multivariate_normal, norm as normal
from statsmodels.tsa.arima_process import arma_generate_sample


class ARMAmodel(ProbabilisticModel, Continuous):

    def __init__(self, parameters, num_AR_params=2, num_MA_params=2, size=100, name='ARMA model'):
        """size is the length of the timeseries to be generated by the model.
        The AR parameters always need to be passed before the MA parameters."""

        if not isinstance(parameters, list):
            raise TypeError('Input of ARMAmodel model is of type list')

        self.size = size
        self.num_AR_params = num_AR_params
        self.num_MA_params = num_MA_params
        self.total_num_params = num_AR_params + num_MA_params

        self._check_input(parameters)

        input_connector = InputConnector.from_list(parameters)
        super().__init__(input_connector, name)

    def _check_input(self, input_values):
        # Check whether input has correct type or format
        if len(input_values) != self.total_num_params:
            raise RuntimeError(
                'Input list must be of length {}, containing {} AR parameters and {} MA parameters'.format(
                    self.total_num_params, self.num_AR_params, self.num_MA_params))

        # Check whether input is from correct domain

        return True

    def _check_output(self, values):
        # f not isinstance(values[0], np.ndarray):
        #   raise ValueError('Output of the normal distribution is always a number.')
        return True

    def get_output_dimension(self):
        return self.size

    def get_number_parameters(self):
        return self.total_num_params

    def forward_simulate(self, input_values, k, rng=np.random.RandomState()):

        self._check_input(input_values)

        results = []
        arparams = np.array(input_values[0:self.num_AR_params])
        maparams = np.array(input_values[self.num_AR_params:self.num_AR_params + self.num_MA_params])
        ar = np.r_[1, -arparams]  # add zero-lag and negate
        ma = np.r_[1, maparams]  # add zero-lag

        for i in range(k):
            # use the random number generators provided as a way to draw the random gaussians. This is for
            # reproducibility. It does not work for statsmodels after 0.9.0
            results.append(arma_generate_sample(ar, ma, self.size, distrvs=rng.randn))

            if np.any(np.isinf(results[-1])):
                warnings.warn("Infinity in forward ARMA model", RuntimeWarning)
            if np.any(np.isnan(results[-1])):
                warnings.warn("nan in forward ARMA model", RuntimeWarning)

        return results


class Autocorrelation(Statistics):
    # another possible implementation is using the function at the following link, however it is slower:
    # https://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.acovf.html

    def __init__(self, order):
        self.order = order

    def statistics(self, data):

        if isinstance(data, list):
            if np.array(data).shape == (len(data),):
                if len(data) == 1:
                    data = np.array(data).reshape(1, 1)
                data = np.array(data).reshape(len(data), 1)
            else:
                data = np.concatenate(data).reshape(len(data), -1)
        else:
            raise TypeError('Input data should be of type list, but found type {}'.format(type(data)))

        results = np.zeros((data.shape[0], self.order))

        data_size = np.int(data.shape[1] / 2)

        for sample in range(data.shape[0]):
            results[sample] = np.correlate(data[sample], data[sample], mode="same")[
                              data_size + 1: data_size + self.order + 1]

        if np.any(np.isinf(results)):
            warnings.warn("Infinity in computation of Autocorrelation statistics", RuntimeWarning)
        if np.any(np.isnan(results)):
            warnings.warn("nan in computation of Autocorrelation statistics", RuntimeWarning)

        results = results / (data.shape[1] - np.arange(1, self.order + 1))
        # rescale the different correlations according to their range
        if np.any(np.isinf(results)):
            warnings.warn("Infinity in computation of Autocorrelation statistics", RuntimeWarning)
        if np.any(np.isnan(results)):
            warnings.warn("nan in computation of Autocorrelation statistics", RuntimeWarning)
        return results


def extract_params_from_journal_ma2(jrnl, step=None):
    params = jrnl.get_parameters(step)
    return np.concatenate((np.array(params['ma1']).reshape(-1, 1), np.array(params['ma2']).reshape(-1, 1)), axis=1)


def extract_params_from_journal_ar2(jrnl, step=None):
    params = jrnl.get_parameters(step)
    return np.concatenate((np.array(params['ar1']).reshape(-1, 1), np.array(params['ar2']).reshape(-1, 1)), axis=1)


def extract_params_and_weights_from_journal_ma2(jrnl, step=None):
    params = jrnl.get_parameters(step)
    weights = jrnl.get_weights(step)
    return np.concatenate((np.array(params['ma1']).reshape(-1, 1), np.array(params['ma2']).reshape(-1, 1)),
                          axis=1), weights.reshape(-1)


def extract_params_and_weights_from_journal_ar2(jrnl, step=None):
    params = jrnl.get_parameters(step)
    weights = jrnl.get_weights(step)
    return np.concatenate((np.array(params['ar1']).reshape(-1, 1), np.array(params['ar2']).reshape(-1, 1)),
                          axis=1), weights.reshape(-1)


def extract_posterior_mean_from_journal_ma2(jrnl, step=None):
    post_mean = jrnl.posterior_mean(step)
    return np.array((post_mean['ma1'], post_mean['ma2']))


def extract_posterior_mean_from_journal_ar2(jrnl, step=None):
    post_mean = jrnl.posterior_mean(step)
    return np.array((post_mean['ar1'], post_mean['ar2']))


# define a theano Op for our likelihood function


def ma2_likelihood(x, theta_1, theta_2):
    cov_matrix = np.zeros(x.shape * 2)
    a = 1 + theta_1 ** 2 + theta_2 ** 2
    cov_matrix[0, 0] = 1
    cov_matrix[1, 1] = 1 + theta_1 ** 2
    # cov_matrix[0, 0] = a
    # cov_matrix[1, 1] = a
    b = theta_1 * (1 + theta_2)
    cov_matrix[0, 1] = theta_1
    cov_matrix[1, 0] = theta_1
    for i in range(2, x.shape[0]):
        cov_matrix[i, i] = a
        cov_matrix[i - 1, i] = b
        cov_matrix[i, i - 1] = b
        cov_matrix[i - 2, i] = theta_2
        cov_matrix[i, i - 2] = theta_2
    return multivariate_normal.logpdf(x, cov=cov_matrix)


def ma2_log_lik_for_mcmc(theta, x_obs, rejection_function=None):
    if rejection_function is not None:
        if not rejection_function(theta):
            return -np.inf  # the log likelihood is -infinity if theta is out of prior region.
    return ma2_likelihood(x_obs, theta[0], theta[1])


def ar2_likelihood(x, theta_1, theta_2):
    partial_ll = np.zeros(len(x))
    partial_ll[0] = normal.logpdf(x[0], loc=0, scale=1)  # this could also be removed, it is constant wrt theta.
    partial_ll[1] = normal.logpdf(x[1], loc=theta_1 * x[0], scale=1)

    # this for loop is quite inefficient.
    for i in range(2, len(x)):
        partial_ll[i] = normal.logpdf(x[i], loc=theta_1 * x[i - 1] + theta_2 * x[i - 2], scale=1)
    return np.sum(partial_ll)


def ar2_likelihood2(x, theta_1, theta_2):
    # as above but faster
    means = np.zeros_like(x)
    means[1] = theta_1 * x[0]
    means[2:] = theta_1 * x[1:-1] + theta_2 * x[0:-2]

    return multivariate_normal.logpdf(x, mean=means)


def ar2_log_lik_for_mcmc(theta, x_obs, rejection_function=None):
    if rejection_function is not None:
        if not rejection_function(theta):
            return -np.inf  # the log likelihood is -infinity if theta is out of prior region.
    return ar2_likelihood2(x_obs, theta[0], theta[1])
